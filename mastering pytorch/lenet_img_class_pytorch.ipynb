{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    " \n",
    "import torch\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.manual_seed(87)\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The mean and std are kept as 0.5 for normalizing pixel values as the pixel values are originally in the range 0 to 1\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomCrop(32, 4),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                           (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join('/Users/pepijnschouten/Desktop/Python_Scripts/' \\\n",
    "        'Python_Scripts_Books/Deep_Learning/Mastering_Pytorch/Datasets/' \\\n",
    "            'CIFAR10', 'data'),\n",
    "    train=True, download=True, transform=train_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=16, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join('/Users/pepijnschouten/Desktop/Python_Scripts/' \\\n",
    "        'Python_Scripts_Books/Deep_Learning/Mastering_Pytorch/Datasets/' \\\n",
    "            'CIFAR10', 'data'),\n",
    "    train=False, download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=10000, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# ordering is important\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (cn1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (cn2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LeNet                                    [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [1, 6, 28, 28]            456\n",
      "├─Conv2d: 1-2                            [1, 16, 10, 10]           2,416\n",
      "├─Linear: 1-3                            [1, 120]                  48,120\n",
      "├─Linear: 1-4                            [1, 84]                   10,164\n",
      "├─Linear: 1-5                            [1, 10]                   850\n",
      "==========================================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 0.31\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.cn1 = nn.Conv2d(3, 6, 5)\n",
    "        self.cn2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cn1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.cn2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = x.view(-1, self.featured_size(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def featured_size(self, x):\n",
    "        size = x.size()[1:]  # all except the first (batch) dimension\n",
    "        return reduce(lambda x, y: x * y, size)\n",
    "\n",
    "lenet = LeNet(num_classes=10).to(device)\n",
    "print(lenet)\n",
    "print(summary(lenet, (1, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optim, epoch):\n",
    "    loss_total = 0.\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        in_img, ground_truth = data\n",
    "        in_img = in_img.to(device)\n",
    "        ground_truth = ground_truth.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        op = net(in_img)\n",
    "        loss = nn.CrossEntropyLoss()(op, ground_truth)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print('[Epoch: {:3}, Mini-batch: {:5}] loss: {:.4f}'.format(epoch + 1,\n",
    "                                             i + 1,\n",
    "                                             loss_total / 200))\n",
    "            loss_total = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for data in testloader:\n",
    "            in_img, ground_truth = data\n",
    "            in_img = in_img.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "            \n",
    "            op = net(in_img)\n",
    "            _, pred = torch.max(op.data, 1)\n",
    "            counter += ground_truth.size(0)\n",
    "            success += (pred == ground_truth).sum().item()\n",
    "\n",
    "    print('LeNet accuracy on 10000 images from test dataset: {:.2f}%'.format(\n",
    "        100 * success / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "[Epoch:   1, Mini-batch:  1000] loss: 9.2186\n",
      "[Epoch:   1, Mini-batch:  2000] loss: 8.1888\n",
      "[Epoch:   1, Mini-batch:  3000] loss: 7.7685\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 45.46%\n",
      "\n",
      "[Epoch:   2, Mini-batch:  1000] loss: 7.4541\n",
      "[Epoch:   2, Mini-batch:  2000] loss: 7.1889\n",
      "[Epoch:   2, Mini-batch:  3000] loss: 6.9728\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 53.90%\n",
      "\n",
      "[Epoch:   3, Mini-batch:  1000] loss: 6.8295\n",
      "[Epoch:   3, Mini-batch:  2000] loss: 6.7022\n",
      "[Epoch:   3, Mini-batch:  3000] loss: 6.6263\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 56.25%\n",
      "\n",
      "[Epoch:   4, Mini-batch:  1000] loss: 6.4609\n",
      "[Epoch:   4, Mini-batch:  2000] loss: 6.5063\n",
      "[Epoch:   4, Mini-batch:  3000] loss: 6.3016\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 58.45%\n",
      "\n",
      "[Epoch:   5, Mini-batch:  1000] loss: 6.2600\n",
      "[Epoch:   5, Mini-batch:  2000] loss: 6.1711\n",
      "[Epoch:   5, Mini-batch:  3000] loss: 6.1169\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 59.79%\n",
      "\n",
      "[Epoch:   6, Mini-batch:  1000] loss: 6.1507\n",
      "[Epoch:   6, Mini-batch:  2000] loss: 6.0019\n",
      "[Epoch:   6, Mini-batch:  3000] loss: 5.9527\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 58.64%\n",
      "\n",
      "[Epoch:   7, Mini-batch:  1000] loss: 6.0256\n",
      "[Epoch:   7, Mini-batch:  2000] loss: 5.8950\n",
      "[Epoch:   7, Mini-batch:  3000] loss: 5.9090\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 61.04%\n",
      "\n",
      "[Epoch:   8, Mini-batch:  1000] loss: 5.8645\n",
      "[Epoch:   8, Mini-batch:  2000] loss: 5.7901\n",
      "[Epoch:   8, Mini-batch:  3000] loss: 5.8676\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 61.97%\n",
      "\n",
      "[Epoch:   9, Mini-batch:  1000] loss: 5.8141\n",
      "[Epoch:   9, Mini-batch:  2000] loss: 5.7417\n",
      "[Epoch:   9, Mini-batch:  3000] loss: 5.7064\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 61.85%\n",
      "\n",
      "[Epoch:  10, Mini-batch:  1000] loss: 5.6741\n",
      "[Epoch:  10, Mini-batch:  2000] loss: 5.5959\n",
      "[Epoch:  10, Mini-batch:  3000] loss: 5.6317\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 62.58%\n",
      "\n",
      "[Epoch:  11, Mini-batch:  1000] loss: 5.5230\n",
      "[Epoch:  11, Mini-batch:  2000] loss: 5.5784\n",
      "[Epoch:  11, Mini-batch:  3000] loss: 5.5594\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 62.01%\n",
      "\n",
      "[Epoch:  12, Mini-batch:  1000] loss: 5.5668\n",
      "[Epoch:  12, Mini-batch:  2000] loss: 5.5312\n",
      "[Epoch:  12, Mini-batch:  3000] loss: 5.4257\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 63.66%\n",
      "\n",
      "[Epoch:  13, Mini-batch:  1000] loss: 5.4378\n",
      "[Epoch:  13, Mini-batch:  2000] loss: 5.4514\n",
      "[Epoch:  13, Mini-batch:  3000] loss: 5.3811\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 64.17%\n",
      "\n",
      "[Epoch:  14, Mini-batch:  1000] loss: 5.4589\n",
      "[Epoch:  14, Mini-batch:  2000] loss: 5.3083\n",
      "[Epoch:  14, Mini-batch:  3000] loss: 5.4001\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 64.23%\n",
      "\n",
      "[Epoch:  15, Mini-batch:  1000] loss: 5.3688\n",
      "[Epoch:  15, Mini-batch:  2000] loss: 5.3013\n",
      "[Epoch:  15, Mini-batch:  3000] loss: 5.3530\n",
      "\n",
      "LeNet accuracy on 10000 images from test dataset: 64.36%\n",
      "\n",
      "Finished Training / Saving model\n"
     ]
    }
   ],
   "source": [
    "# define optimizer\n",
    "optim = torch.optim.Adam(lenet.parameters(), lr=0.001)\n",
    "\n",
    "# training loop over the dataset multiple times\n",
    "epochs = 15\n",
    "print('Started Training')\n",
    "for epoch in range(epochs):  \n",
    "    train(lenet, trainloader, optim, epoch)\n",
    "    print()\n",
    "    test(lenet, testloader)\n",
    "    print()\n",
    "\n",
    "print('Finished Training / Saving model')\n",
    "model_path = os.path.join(\n",
    "    '/Users/pepijnschouten/Desktop/Python_Scripts/' \\\n",
    "        'Python_Scripts_Books/Deep_Learning/' \\\n",
    "            'Mastering_Pytorch/Own_Files/Ch3_Deep_CNNs/' \\\n",
    "                'model_dir',\n",
    "    'lenet_cifar_model.pth')\n",
    "torch.save(lenet.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test dataset images\n",
    "d_iter = iter(testloader)\n",
    "im, ground_truth = next(d_iter)\n",
    "\n",
    "# load model\n",
    "lenet_cached = LeNet(num_classes=10)\n",
    "lenet_cached.load_state_dict(torch.load(model_path,\n",
    "                                        weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on 10000 images from test dataset: 64 %\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        im, ground_truth = data\n",
    "        op = lenet_cached(im)\n",
    "        _, pred = torch.max(op.data, 1)\n",
    "        counter += ground_truth.size(0)\n",
    "        success += (pred == ground_truth).sum().item()\n",
    "\n",
    "print('Model accuracy on 10000 images from test dataset: %d %%' % (\n",
    "    100 * success / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for class plane : 73 %\n",
      "Model accuracy for class   car : 91 %\n",
      "Model accuracy for class  bird : 55 %\n",
      "Model accuracy for class   cat : 39 %\n",
      "Model accuracy for class  deer : 57 %\n",
      "Model accuracy for class   dog : 52 %\n",
      "Model accuracy for class  frog : 81 %\n",
      "Model accuracy for class horse : 66 %\n",
      "Model accuracy for class  ship : 69 %\n",
      "Model accuracy for class truck : 56 %\n"
     ]
    }
   ],
   "source": [
    "class_success = list(0. for i in range(10))\n",
    "class_counter = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        im, ground_truth = data\n",
    "        op = lenet_cached(im)\n",
    "        _, pred = torch.max(op, 1)\n",
    "        c = (pred == ground_truth).squeeze()\n",
    "        for i in range(10000):\n",
    "            ground_truth_curr = ground_truth[i]\n",
    "            class_success[ground_truth_curr] += c[i].item()\n",
    "            class_counter[ground_truth_curr] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Model accuracy for class %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_success[i] / class_counter[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
