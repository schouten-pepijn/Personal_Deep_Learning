{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super(LM, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(120, 120),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(120, 120)\n",
    "                ) for _ in range(num)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = [fc(x) for fc in self.fcs]\n",
    "        \n",
    "        output = torch.stack(x, dim=1)\n",
    "        \n",
    "        # output = torch.sum(output, dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class SM(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super(SM, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.num = num\n",
    "        \n",
    "        self.fc1 = nn.Linear(120, 120)\n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        \n",
    "        self.w1 = nn.Parameter(\n",
    "            torch.block_diag(\n",
    "                *[self.fc1.weight] * num\n",
    "            )\n",
    "        )    \n",
    "        self.b1 = nn.Parameter(\n",
    "            torch.cat([self.fc1.bias] * num)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.w2 = nn.Parameter(\n",
    "            torch.block_diag(\n",
    "                *[self.fc2.weight] * num\n",
    "            )\n",
    "        )                   \n",
    "        self.b2 = nn.Parameter(\n",
    "            torch.cat([self.fc2.bias] * num)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.repeat(1, self.num)\n",
    "        \n",
    "        x = torch.matmul(x, self.w1) + self.b1\n",
    "        x = torch.relu(x)\n",
    "        x = torch.matmul(x, self.w2) + self.b2\n",
    "        output = x.view(-1, self.num, 120)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "layers = 10\n",
    "inputs = torch.randn(100, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LM                                       [100, 10, 120]            --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Sequential: 2-1                   [100, 120]                --\n",
      "│    │    └─Linear: 3-1                  [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-2                    [100, 120]                --\n",
      "│    │    └─Linear: 3-3                  [100, 120]                14,520\n",
      "│    └─Sequential: 2-2                   [100, 120]                --\n",
      "│    │    └─Linear: 3-4                  [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-5                    [100, 120]                --\n",
      "│    │    └─Linear: 3-6                  [100, 120]                14,520\n",
      "│    └─Sequential: 2-3                   [100, 120]                --\n",
      "│    │    └─Linear: 3-7                  [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-8                    [100, 120]                --\n",
      "│    │    └─Linear: 3-9                  [100, 120]                14,520\n",
      "│    └─Sequential: 2-4                   [100, 120]                --\n",
      "│    │    └─Linear: 3-10                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-11                   [100, 120]                --\n",
      "│    │    └─Linear: 3-12                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-5                   [100, 120]                --\n",
      "│    │    └─Linear: 3-13                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-14                   [100, 120]                --\n",
      "│    │    └─Linear: 3-15                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-6                   [100, 120]                --\n",
      "│    │    └─Linear: 3-16                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-17                   [100, 120]                --\n",
      "│    │    └─Linear: 3-18                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-7                   [100, 120]                --\n",
      "│    │    └─Linear: 3-19                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-20                   [100, 120]                --\n",
      "│    │    └─Linear: 3-21                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-8                   [100, 120]                --\n",
      "│    │    └─Linear: 3-22                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-23                   [100, 120]                --\n",
      "│    │    └─Linear: 3-24                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-9                   [100, 120]                --\n",
      "│    │    └─Linear: 3-25                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-26                   [100, 120]                --\n",
      "│    │    └─Linear: 3-27                 [100, 120]                14,520\n",
      "│    └─Sequential: 2-10                  [100, 120]                --\n",
      "│    │    └─Linear: 3-28                 [100, 120]                14,520\n",
      "│    │    └─ReLU: 3-29                   [100, 120]                --\n",
      "│    │    └─Linear: 3-30                 [100, 120]                14,520\n",
      "==========================================================================================\n",
      "Total params: 290,400\n",
      "Trainable params: 290,400\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 29.04\n",
      "==========================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.92\n",
      "Params size (MB): 1.16\n",
      "Estimated Total Size (MB): 3.13\n",
      "==========================================================================================\n",
      "0.6842830181121826\n"
     ]
    }
   ],
   "source": [
    "lm = LM(num=layers)\n",
    "\n",
    "print(summary(lm, input_data=inputs))\n",
    "\n",
    "st = time()\n",
    "for _ in range(epochs):\n",
    "    outputs = lm(inputs)\n",
    "\n",
    "print(time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "SM                                       [100, 10, 120]            2,911,440\n",
      "==========================================================================================\n",
      "Total params: 2,911,440\n",
      "Trainable params: 2,911,440\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.05\n",
      "==========================================================================================\n",
      "1.2347021102905273\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(100, 120)\n",
    "sm = SM(num=layers)\n",
    "\n",
    "print(summary(sm, input_data=inputs))\n",
    "\n",
    "\n",
    "st = time()\n",
    "for _ in range(1000):\n",
    "    outputs = sm(inputs)\n",
    "\n",
    "print(time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 120])\n",
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "w_b = [(x.weight, x.bias) for x in [nn.Linear(120, 120) for _ in range(2)]]\n",
    "W = torch.block_diag(*[w for w, _ in w_b])\n",
    "\n",
    "W = torch.cat([w for w, _ in w_b], dim=0)\n",
    "\n",
    "print(W.shape)\n",
    "\n",
    "b = nn.Parameter(torch.cat([b for _, b in w_b]))\n",
    "\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
